{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://github.com/m2dsupsdlclass/lectures-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Backpropagation? \n",
    "\n",
    "Deep neural networks involve a huge number of parameters, corresponding to the weights and the biases appearing in the definition of the network. Given a training sample, all these parameters are  estimated by minimizing an empirical loss function. The function to minimize is generally very complex and  not convex. \n",
    "The minimization of the loss function is done via an optimization algorithm such as the **Stochastic Gradient Descent **(SGD) algorithm or more recent variants. All these algorithms require at each step the computation of the gradient of the loss function.  \n",
    "The **backpropagation algorithm** (Rumelhart et al, 1986) is a method for **computing the gradient of the loss function**, in order to estimate the parameters of a - possibly deep - neural network. It is composed of a succession of a *forward* pass and a *backward* pass through the network in order to compute the gradient. It can be easily parallelisable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this TP are to : \n",
    "   * Understand the theory of the backpropagation algorithm\n",
    "   * Implement logistic regression and multi layers perceptron algorithms using backpropagation equations with numpy\n",
    "   * Use Keras to apply the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Dataset\n",
    "\n",
    "\n",
    "The dataset we used is composed of 8x8 pixel images of hand written digits available within sklearn library.\n",
    "\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "N = reduce(lambda x,y: x*y,digits.images[0].shape)\n",
    "print(\"Image dimension : N=%d\"%N)\n",
    "K = len(set(digits.target))\n",
    "print(\"Number of classes : K=%d\"%K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "fig =plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "ax.set_title(\"image label: %d\" % digits.target[sample_index])\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data dimension and type\")\n",
    "print(\"X_train : \" + str(X_train.shape) + \", \" +str(X_train.dtype))\n",
    "print(\"y_train : \" + str(y_train.shape) + \", \" +str(y_train.dtype))\n",
    "print(\"X_test : \" + str(X_test.shape) + \", \" +str(X_test.dtype))\n",
    "print(\"y_test : \" + str(y_test.shape) + \", \" +str(y_test.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Utils Function\n",
    "\n",
    "Write utils function that will be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding function\n",
    "\n",
    "$$\n",
    "OneHotEncoding(N_{class},i=4) = \n",
    "\\begin{bmatrix}\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  1\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $N_{class}=10$ and $i \\in [0,9]$\n",
    "\n",
    "**Exercise :** Implement the **one hot encoding** function of an integer array for a fixed number of classes (similar to keras' `to_categorical`):  \n",
    "Ensure that your function works for several vectors at a time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the one_hot function\n",
    "def one_hot(n_classes,y):\n",
    "    ##\n",
    "    return ohy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/one_hot_encoding.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the solution works on 1D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohy = one_hot(y=3,n_classes=10)\n",
    "print(\"Expected : [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] \\n\")\n",
    "print(\"Computed  :\" + str(ohy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the solution works on 2D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohY = one_hot(n_classes=10, y=[0, 4, 9, 1])\n",
    "print(\"Expected : [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] \\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \\n  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\")\n",
    "print(\"Computed  :\" + str(ohY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Exercise :** Implement the softmax function.  \n",
    "Ensure that your function works for several vectors at a time.  \n",
    "Hint : use the *axis* and *keepdims* argument of the numpy function `np.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keepdims option\n",
    "x = np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "\n",
    "print(\"Sum all elements of array :\")\n",
    "sx = np.sum(x)\n",
    "print(sx)\n",
    "\n",
    "print(\"Sum all elements over axis (dimension) :\" )\n",
    "sx = np.sum(x, axis=-1)\n",
    "print(str(sx), str(\", Dimension :\") ,str(sx.shape))\n",
    "\n",
    "print(\"Sum all elements over axis and with keepdims (dimension) :\" )\n",
    "sx = np.sum(x, axis=-1,  keepdims=True)\n",
    "print(str(sx), str(\", Dimension :\") ,str(sx.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the softmax function\n",
    "def softmax(x):\n",
    "    ###\n",
    "    return softmaxX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your function works for a 1D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 2, -3]\n",
    "sx = softmax(x)\n",
    "print(\"Expected : [9.99662391e-01 3.35349373e-04 2.25956630e-06]\")\n",
    "print(\"Computed \" + str(sx))\n",
    "print(\"Value Sum to one : %d\" %np.sum(sx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your function works for a  2D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "sX = softmax(X)\n",
    "print(\"Expected : [[9.99662391e-01 3.35349373e-04 2.25956630e-06] \\n [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\")\n",
    "print(\"Value found\" + str(sX))\n",
    "print(\"Value Sum to one : \" + str(np.sum(sX, axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "We consider the loss function associated to the cross-entropy. Minimizing this loss function corresponds to minimization of the negative log likelihood  (which is equivalent to the  maximization of the log likelihood).\n",
    "\n",
    "According to course's notations, we have \n",
    "$$ \\ell(f(x),y) = -\\log (f(x))_y = -  \\sum_{k=1}^K  \\mathbb{1}_{y=k} \\log (f(x))_k\n",
    "$$\n",
    "\n",
    "where $(f(x))_k =  \\mathbb{P}(Y=k~/~x)$, the predicted probability for the class $k$, when the input equals $x$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**:  \n",
    "Write a function that computes the mean negative likelihood (empirical loss) of a group of observations `Y_true` and `Fx`, where `Y_true` and `Fx` are respectively the one-hot encoded representation of the observed labels and the predictions for the associated inputs $x$ i.e. :\n",
    "\n",
    "* `Y_true` is the one-hot encoded representation of $y$\n",
    "* `Fx` is the output of a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the negative_log_likelihood function\n",
    "EPSILON = 1e-8\n",
    "def NegLogLike(Y_true, Fx):\n",
    "    ###\n",
    "    return nll_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/negative_log_likelihood_function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the loss function  for a single observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple case\n",
    "ohy_true = [1, 0, 0]\n",
    "fx = [.99, 0.01, 0] \n",
    "nll1 = NegLogLike(ohy_true, fx)\n",
    "print(\"A small value for the loss function :\")\n",
    "print(\"Expected value : 0.01005032575249135\")\n",
    "print(\"Computed : \" + str(nll1) )\n",
    "\n",
    "# Case with bad prediction\n",
    "ohy_true = [1, 0, 0]\n",
    "fx = [0.01, .99, 0] \n",
    "nll2 = NegLogLike(ohy_true, fx)\n",
    "print(\"Higher value for the loss function):\")\n",
    "print(\"Expected value : 4.605169185988592\")\n",
    "print(\"Computed : \" + str(nll2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can handle the case where $  (f(x))_y =0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero case\n",
    "ohy_true = [1, 0, 0]\n",
    "fx = [0, 0.01, 0.99] \n",
    "nll3 = NegLogLike(ohy_true, fx)\n",
    "print(\"Expected value : 18.420680743952367\")\n",
    "\n",
    "print(\"Computed : \" + str(nll3) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the  empirical loss for several observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohY_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Fx = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "nll4 = NegLogLike(ohY_true, Fx)\n",
    "\n",
    "print(\"Expected value : 0.0033501019174971905\")\n",
    "print(\"Computed : \" + str(nll4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    ###\n",
    "    return sigX\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    ###\n",
    "    return dsig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the sigmoid function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "ax.plot(x, sigmoid(x), label='sigmoid')\n",
    "ax.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD **one observation at a time** (On-line gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Complete the `LogisticRegression` class by following these steps (Use the functions you have written above) :\n",
    "\n",
    "**Notation** : $x \\in \\mathbb{R}^N$, $y \\in [0,...,K]$, $W \\in \\mathbb{R}^{K,N}$, $b \\in \\mathbb{R}^K$\n",
    "\n",
    "\n",
    "1.  Implement the `forward` function which computes the prediction of the model for the input $x$:  \n",
    "$$f(x) = softmax(\\mathbf{W} x + b)$$\n",
    "\n",
    "2. Implement the `grad_loss` function which computes the gradient of the loss function  $  \\ell(f(x),y) = -\\log (f(x))_y $ (for an input $x$ and its corresponding observed output $y$) with respect to the parameters of the model $W$ and $b$ :\n",
    "\n",
    "\\begin{array}{ll} \n",
    "   grad_W &= \\frac{d}{dW} [-\\log (f(x))_y] \\\\\n",
    "   grad_b &= \\frac{d}{db} [-\\log (f(x))_y]\n",
    "\\end{array}\n",
    "\n",
    "**Hint**  \n",
    "\\begin{array}{ll}\n",
    "    \\frac{d}{dW_{i,j}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      [f(x)_{y}-1]*x_j, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}*x_j, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{db_{i}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      f(x)_{y}-1, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "\\end{array}\n",
    "       \n",
    "\n",
    "3. Implement the `train` function which uses the grad function output to update $\\mathbf{W}$ and $b$ with traditional SGD update without momentum :\n",
    "\\begin{array}{ll}\n",
    "W &= W - \\varepsilon \\frac{d}{dW} [-\\log (f(x))_y]\\\\\n",
    "b &= b - \\varepsilon \\frac{d}{db} [-\\log (f(x))_y]\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        ###\n",
    "        return sZ\n",
    "    \n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        ###\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        ### \n",
    "    \n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        nll = NegLogLike(one_hot(self.output_size, y), self.forward(x))\n",
    "        return nll\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        acc = np.mean(y_preds == y)\n",
    "        return acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lr_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "lr = LogisticRegression(N, K)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx].reshape(1,-1)).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "    ax0.grid(False)\n",
    "    ax0.axis('off')\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Multi Layer Perceptron\n",
    "\n",
    "\n",
    "In this section we consider a neural network model with one hidden layer using the sigmoid activation function.\n",
    "You will implement the backpropagation algorithm (with the chain rule). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Complete the `NeuralNet` class following these steps :\n",
    "\n",
    "**Notation** : $x \\in \\mathbb{R}^N$, $h \\in \\mathbb{R}^H$, $y \\in [0,...,K]$, $W^{h} \\in \\mathbb{R}^{H,N}$, $b^h \\in \\mathbb{R}^H$, $W^{o} \\in \\mathbb{R}^{K,H}$, $b^o \\in \\mathbb{R}^K$\n",
    "\n",
    "\n",
    "1. Implement the `forward` function for a model with one hidden layer with a sigmoid activation function:\n",
    "\\begin{array}{lll} \n",
    "  \\mathbf{h} &= sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h}) &= sigmoid(z^h(x)) \\\\\n",
    "  f(x) &= softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o}) &= softmax(z^o(x))\\\\\n",
    "\\end{array}\n",
    "\n",
    "  which returns $y$ if *keep_activation* = False and $y$, $h$ and  $z^h(x)$ otherwise (we keep all the intermediate values).\n",
    "  \n",
    "2.  Implement the `grad_loss` function which computes the gradient of the loss function (for an  $x$ and its corresponding observed  output $y$) with respect to the parameters of the network $W^h$, $b^h$, $W^o$ and $b^o$ :\n",
    "\n",
    "\\begin{array}{ll} \n",
    "   \\nabla_{W^{o}}loss &= \\frac{d}{dW^{o}} [-\\log (f(x))_y] \\\\\n",
    "   \\nabla_{b^{o}}loss &= \\frac{d}{db^{o}} [-\\log (f(x))_y] \\\\\n",
    "   \\nabla_{W^{h}}loss &= \\frac{d}{dW^{h}} [-\\log (f(x))_y] \\\\\n",
    "   \\nabla_{b^{h}}loss &= \\frac{d}{db^{h}} [-\\log (f(x))_y]\n",
    "\\end{array}\n",
    "\n",
    "**Hint**  \n",
    "\n",
    "\\begin{array}{ll}\n",
    "    \\frac{d}{dz^0_{i}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      f(x)_{y}-1, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{dW^o_{i,j}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      [f(x)_{y}-1]*h_j, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}*h_j, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{db^o_{i}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      f(x)_{y}-1, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{dh_{j}} [-\\log (f(x))_y] &= \\nabla_{z^{o}}loss ~\\cdot~ W^o_{.,j} \\\\\n",
    "    \\frac{d}{dz^h_{j}} [-\\log (f(x))_y] &=  \\nabla_{z^{o}}loss ~\\cdot~ W^o_{.,j}  * dsigmoid(z^h_{j}) \\\\\n",
    "    \\frac{d}{dW^h_{j,l}} [-\\log (f(x))_y] &= \\nabla_{z^h}loss_j  *  x_l \\\\ \n",
    "    \\frac{d}{db^h_{j}} [-\\log (f(x))_y] &= \\nabla_{z^h}loss_j  \\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(\n",
    "            size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(\n",
    "            size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o = np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    \n",
    "    def forward(self, X, keep_activation=False):\n",
    "        ###\n",
    "        rep = [fx, h, z_h] if keep_activation else fx\n",
    "        return rep\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        fx = self.forward(X)\n",
    "        ohy = one_hot(self.output_size, y)\n",
    "        nll = NegLogLike(ohy, fx)\n",
    "        return nll \n",
    "\n",
    "    def grad_loss(self, X, y_true):\n",
    "        ####\n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        fx = self.forward(X)\n",
    "        if len(X.shape) == 1:\n",
    "            \n",
    "            yp = np.argmax(fx)\n",
    "        else:\n",
    "            yp = np.argmax(fx, axis=1)\n",
    "        return yp\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/nn_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 10\n",
    "model = NeuralNet(N, H, K)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = model.loss(X_train, y_train)\n",
    "train_acc = model.accuracy(X_train, y_train)\n",
    "test_acc = model.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model for several epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, losses_test, accuracies, accuracies_test = [], [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "losses_test.append(model.loss(X_test, y_test))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    losses_test.append(model.loss(X_test, y_test))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss evolution per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(losses,label='train')\n",
    "ax.plot(losses_test,label='test')\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend(loc='best');\n",
    "ax.set_title(\"Training loss\");\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(accuracies, label='train')\n",
    "ax.plot(accuracies_test, label='test')\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.legend(loc='best');\n",
    "ax.set_title(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Tensorflow/keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the same multi layer perceptron using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "n_features = 64\n",
    "n_classes = 10\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mlp_keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a function that produces the same results that the plot_prediction function but with keras model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/plot_prediction_keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare loss and accuracy of keras and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/compare_loss_acc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
