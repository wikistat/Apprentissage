{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scénario d'Apprentissage Statistique](https://github.com/wikistat/Apprentissage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation de données d'enquête en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a>:  prévision du seuil de revenu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Des données publiques disponibles sur le site [UCI repository](http://archive.ics.uci.edu/ml/) sont extraites de la base de données issue du recensement réalisé aux Etats Unis en 1994. Ces données son largement utilisées et font référence comme outil de *benchmark* pour comparer les performances de méthodes d’apprentissage ou modélisation statistique. L’objectif est  de prévoir la variable binaire « revenu annuel » (`income`) supérieur ou inférieur à 50k$. Il ne s’agit pas encore de données massives mais 32561 individus sont décrits par les 14 variables du tableau ci-dessous :\n",
    "\n",
    "Num| Libellé |\tEnsemble de valeurs\n",
    "-|--|--|--\n",
    "1|`Age`|\treal\n",
    "2|\t`workClass`|\tPrivate, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "3|\t`fnlwgt`|\treal\n",
    "4|\t`education`|\tBachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool\n",
    "5|\t`educNum`|\tinteger\n",
    "6|\t`mariStat`|\tMarried-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\n",
    "7|\t`occup`|\tTech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\n",
    "8|\t`relationship`|\tWife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "9|\t`origEthn`|\tWhite, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "10|\t`sex`|\tFemale, Male\n",
    "11|\t`capitalGain`|\treal  \n",
    "12|\t`capitalLoss`|\treal\n",
    "13|\t`hoursWeek`|\treal\n",
    "14|\t`nativCountry`|\tUnited-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands\n",
    "15|\t`income`|\t\tincHigh (>50K), incLow (<=50K)\n",
    "\n",
    "Une première étape permettant de vérifier, sélectionner, recoder les données, a permis de construire un fichier de type `.csv` qui est utilisé dans ce calepin.\n",
    "\n",
    "\n",
    "**Répondre aux questions en s'aidant des résultats des exécutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Préparation des données\n",
    "### 1.1 Lecture  \n",
    "\n",
    "Charger les [données](https://www.math.univ-toulouse.fr/~besse/Wikistat/data/adultCensus.csv)  dans le répertoire courant (`path=\"\"`) ou exécuter directement la cellule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Importations \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path=\"\"  # si les données sont déjà dans le répertoire courant\n",
    "#path=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/data/\"\n",
    "datBase=pd.read_csv(path+'adultCensus.csv')\n",
    "datBase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repérer les variables quantitatives, qualitatives. Certaines (`age`, `hoursWeek`) sont présentes sous les deux types. Beaucoup de modalités on déjà été regroupées (voir le programme) certaines variables sont rendues qualitatives (`capitalLoss` ou `Gain`) et transformées par une fonction *log*. La variable à modéliser est `income`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 Exploration élémentaire et vérifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beaucoup de variables qualitatives\n",
    "datBase.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pas de données manquantes\n",
    "datBase.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la distribution de la variable `age`, de celle `income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase[\"age\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase[\"fnlwgt\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase[\"relationship\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase.plot(kind=\"scatter\",x=\"age\",y=\"educNum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire des liaisons : `age x hoursWeek`, `age x income`, `sex x income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase.plot(kind=\"scatter\",x=\"hoursWeek\",y=\"age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase.boxplot(column=\"age\",by=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** La variable `fnlwgt` (Final sampling weight:  Inverse of sampling fraction adjusted for non-response and over or under sampling of particular groups) est assez obscure.  Que dire de sa liaison avec la variable cible? Elle est supprimée par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBase.boxplot(column=\"fnlwgt\",by=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mosaic plot\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "mosaic(datBase,[\"income\",\"sex\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Variables indicatrices\n",
    "**Q** Pourquoi l’introduction de dummy variables ? Pour quelles méthodes cela serait-il aussi nécessaire en R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datBaseDum=pd.get_dummies(datBase[[\"workClass\",\"education\",\"mariStat\",\n",
    "    \"occup\",\"relationship\",\"origEthn\",\"sex\",\"capitalGain\",\"capitalLoss\",\n",
    "    \"nativCountry\",\"ageQ\",\"hoursWeekQ\"]])\n",
    "datBaseDum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = datBase[[\"age\",\"educNum\",\"hoursWeek\",\"LcapitalGain\",\n",
    "             \"LcapitalLoss\",\"income\"]].join(datBaseDum)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable cible\n",
    "Y=datBase[\"income\"]\n",
    "# Variables prédictives\n",
    "X=X.drop([\"income\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Extraction des échantillons\n",
    "**Q** Quel est l’objectif de cette cellule ? Justifier la nécessité de ce traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=3000,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Méthodes de modélisation \n",
    "L'algorithme des *k* plus proches voisins fourni des réultats assez catastrophiques sur ces données, il n'est pas repris. Est-ce dû au nombre important d'indicatrices dans les variables explicatives? A confirmer sur d'autres exemples.\n",
    "### 2.1 Régression logistique\n",
    "**Q** Commenter  les options de la commande  `GridSearchCV`. A quoi sert `param` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "tps0=time.clock()\n",
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "param=[{\"C\":[0.1, 0.15,0.2,0.25,0.3]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\"), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(X_train, Y_train)  # GridSearchCV est lui même un estimateur\n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps logit = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                              1.-logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur sur l'échantillon test\n",
    "1-logitOpt.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prévision\n",
    "y_chap = logitOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** La matrice de confusion n’est pas symétrique. Quelle pourrait en être la raison ?\n",
    "\n",
    "**Q** Quels algorithmes pourraient être exécutés en R pour la régression logistique? Quelles informations complémentaires en tirer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "LogisticRegression(penalty=\"l1\",C=logitOpt.best_params_['C']).fit(X_train, Y_train).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "model = LogisticRegressionCV(Cs=10,cv=5, penalty=\"l1\",\n",
    "        n_jobs=-1,random_state=13,solver=\"liblinear\").fit(X_train,Y_train)\n",
    "m_log_alphas = -np.log10(model.Cs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(m_log_alphas, model.scores_['incLow'].T, ':')\n",
    "plt.plot(m_log_alphas, model.scores_['incLow'].T.mean(axis=-1), 'k',\n",
    "         label='precision moyenne', linewidth=2)\n",
    "plt.axvline(-np.log10(model.C_), linestyle='--', color='k',\n",
    "            label='Cs: optimal par VC')\n",
    "plt.legend()\n",
    "plt.xlabel('-log(CS)')\n",
    "plt.ylabel(u'Précision')\n",
    "plt.title(u'Précision de chaque validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que représente le graphique ? Comment l’interpréter ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Arbre binaire de discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tps0=time.clock()\n",
    "# Optimisation de la profondeur de l'arbre\n",
    "param=[{\"max_depth\":list(range(2,10))}]\n",
    "tree= GridSearchCV(DecisionTreeClassifier(),param,cv=10,n_jobs=-1)\n",
    "treeOpt=tree.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps arbre = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                             1. - treeOpt.best_score_,treeOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision\n",
    "1-treeOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = treeOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "treeOpt.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydot\n",
    "treeG=DecisionTreeClassifier(max_depth=treeOpt.best_params_['max_depth'])\n",
    "treeG.fit(X_train,Y_train)\n",
    "dot_data = StringIO() \n",
    "export_graphviz(treeG, out_file=dot_data) \n",
    "graph=pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_png(\"treeOpt.png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='treeOpt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle est l’insuffisance de l’implémentation des arbres de décision dans Scikit-learn  par rapport à celle de rpart de R ? Que dire de l’arbre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xnet_train = scaler.transform(X_train)  \n",
    "# Meme transformation sur le test\n",
    "Xnet_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tps0=time.clock()\n",
    "param_grid=[{\"hidden_layer_sizes\":list([(5,),(6,),(7,),(8,)])}]\n",
    "nnet= GridSearchCV(MLPClassifier(max_iter=500),param_grid,cv=10,n_jobs=-1)\n",
    "nnetOpt=nnet.fit(Xnet_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps perceptron = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                    1. - nnetOpt.best_score_,nnetOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle stratégie d’optimisation est adoptée ? Quelle autre pourrait l’être? Quel réseau pourrait également être pris en compte? Quelles sont les fonctions d’activation des neurones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision sur le test\n",
    "1-nnetOpt.score(Xnet_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = nnetOpt.predict(Xnet_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Forêts aléatoires\n",
    "**Q** Commenter les choix de tous les paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# définition des paramètres\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=2, min_samples_leaf=1, \n",
    "   max_features='auto', max_leaf_nodes=None,\n",
    "   bootstrap=True, oob_score=True)\n",
    "# apprentissage\n",
    "rfFit = forest.fit(X_train,Y_train)\n",
    "print(1-rfFit.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfFit.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimisation du paramètre\n",
    "tps0=time.clock()\n",
    "param=[{\"max_features\":list(range(2,10,1))}]\n",
    "rf= GridSearchCV(RandomForestClassifier(n_estimators=100),param,cv=10,n_jobs=-1)\n",
    "rfOpt=rf.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps r forest = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                    1. - rfOpt.best_score_,rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prévision\n",
    "y_chap = rfFit.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier(n_estimators=100,max_features=2)\n",
    "rfFit=rf.fit(X_train, Y_train)\n",
    "# Importance décroissante des variables\n",
    "importances = rfFit.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(20):\n",
    "    print(X_train.columns[indices[f]], importances[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graphe des importances\n",
    "plt.figure()\n",
    "plt.title(\"Importances des variables\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment est obtenu le graphique ? Quelle importance ? Comment interpréter ces résultats ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Gradient boosting\n",
    "**Q** Pourquoi pas de paramètre `njobs=-1`? \n",
    "**Q** En plus de celui `optimiser, quels sont les 2 principaux paramètres cet algorithme laissés par défaut ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "tps0=time.clock()\n",
    "param=[{\"n_estimators\":[100, 150, 200, 250]}]\n",
    "gbm= GridSearchCV(GradientBoostingClassifier(),param,cv=10)\n",
    "gbmOpt=gbm.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.clock()-tps0)/60\n",
    "print(\"Temps boosting = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                        1. - gbmOpt.best_score_,gbmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-gbmOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = gbmOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Comparaison des méthodes\n",
    "\n",
    "### 3.1 Courbes ROC\n",
    "**Q** En cohérence avec les résultats précédents, quelle est la courbe la plus au dessus des autres? Commenter ce graphique, que signifie AUC ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod=[[\"GBM\",gbmOpt],[\"RF\",rfOpt],[\"NN\",nnetOpt],[\"Tree\",treeOpt],[\"Logit\",logitOpt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modèles tous réajustés sur les mêmes variables centrées réduites\n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].fit(Xnet_train, Y_train).predict_proba(Xnet_test)\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,probas_[:,1], pos_label=\"incLow\")\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0]),\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validation croisée *Monte Carlo*\n",
    "**Q** Quelle différence entre la validation croisée Monte Carlo et la V-fold cross validation?\n",
    "\n",
    "**Q** Les variables sont « standardisées ». Pourquoi ? Est-ce important et pour quelles méthodes? Commenter les résultats. Quelle méthode choisir ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "import time\n",
    "tps0=time.clock()\n",
    "check_random_state(11)\n",
    "# définition des estimateurs\n",
    "logit= LogisticRegression(penalty=\"l1\")\n",
    "tree = DecisionTreeClassifier()\n",
    "nnet = MLPClassifier(max_iter=400)\n",
    "rf   = RandomForestClassifier(n_estimators=200)\n",
    "gbm  = GradientBoostingClassifier()\n",
    "# Nombre d'itérations\n",
    "B=10 # pour tester le programme, mettre plutôt B=3\n",
    "# définition des grilles de paramètres\n",
    "listMethGrid=[[gbm,{\"n_estimators\":[100, 150, 200, 250]}],\n",
    "    [rf,{\"max_features\":list(range(6,10))}],\n",
    "    [nnet,{\"hidden_layer_sizes\":list([(5,),(6,),(7,),(8,)])}],\n",
    "    [tree,{\"max_depth\":list(range(2,10))}],\n",
    "    [logit,{\"C\":[0.1, 0.15,0.2,0.25,0.3]}]]\n",
    "# Initialisation à 0 \n",
    "arrayErreur=np.empty((B,5))\n",
    "for i in range(B):   # itérations sur B échantillons test\n",
    "    # extraction apprentissage et test\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=1000)\n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X_train)  \n",
    "    Xnet_train = scaler.transform(X_train)  \n",
    "    # Meme transformation sur le test\n",
    "    Xnet_test = scaler.transform(X_test)\n",
    "    # optimisation de chaque méthode et calcul de l'erreur sur le test\n",
    "    for j,(method, grid_list) in enumerate(listMethGrid):\n",
    "        methodGrid=GridSearchCV(method,grid_list,cv=10,n_jobs=-1).fit(X_train, Y_train)\n",
    "        methodOpt = methodGrid.best_estimator_\n",
    "        methFit=methodOpt.fit(Xnet_train, Y_train)\n",
    "        arrayErreur[i,j]=1-methFit.score(Xnet_test,Y_test)\n",
    "tps1=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframeErreur=pd.DataFrame(arrayErreur,columns=[\"GBM\",\"RF\",\"NN\",\"Tree\",\"Logit\"])\n",
    "print(\"Temps execution en mn :\",(tps1 - tps0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframeErreur[[\"GBM\",\"RF\",\"NN\",\"Tree\",\"Logit\"]].boxplot(return_type='dict')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Moyennes\n",
    "dataframeErreur.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Q** Les SVM ne font pas partie de la comparaison à cause de temps rédhibitoires d’exécution. A quoi est-ce dû ? Commenter les temps d’exécution des différentes étapes. Quelle autre package pourrait être utilisé pour la section 2.5 ? Pourquoi ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
